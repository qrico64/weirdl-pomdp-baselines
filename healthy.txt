Confirmed healthy 1 :D

> /mmfs1/gscratch/weirdlab/qirico/Meta-Learning-25-10-1/weirdl-pomdp-baselines-cp38/policies/models/transformer_encoder_actor.py(241)act()
-> mask = self.mask[:T, :T]
(Pdb) p rewards.shape
torch.Size([300, 1, 1])
(Pdb) p rewards[:100].sum()
tensor(-35.622, device='cuda:0')
(Pdb) p rewards[:100].mean()
tensor(-0.356, device='cuda:0')
(Pdb) p rewards[:200].mean()
tensor(-0.168, device='cuda:0')
(Pdb) p rewards[:300].mean()
tensor(-0.946, device='cuda:0')
(Pdb) p nominals[199:201]
tensor([[0],
        [0]], device='cuda:0')
(Pdb) p nominals[199:202]
tensor([[0],
        [0],
        [1]], device='cuda:0')
(Pdb) p obs[200,0]
tensor([     0.548,      0.953,     -0.045,     -0.063,     -0.293,     -0.081,
             1.186,      0.525,     -0.570,      0.521,     -0.898,      0.001,
             1.142,     -0.179,     -0.122,     -0.036,     -0.093,     -0.107,
            -0.562,      1.415,     -0.316,     -0.024,      0.336,     -0.144,
            -0.222,      1.273,     -2.202,      1.000], device='cuda:0')
(Pdb) p obs[201,0]
tensor([ 0.704,  0.994, -0.068, -0.075,  0.044,  0.025,  0.024,  0.079, -0.012,
        -0.002, -0.021, -0.089,  0.027,  0.014,  0.042, -0.095,  0.039, -0.025,
        -0.061,  0.106,  0.274, -0.117,  0.080, -0.088, -0.054,  0.045, -0.027,
         0.000], device='cuda:0')
(Pdb) p prev_actions[200,0]
tensor([ 0.073, -0.226,  0.087,  0.030,  0.098,  0.160, -0.018,  0.088],
       device='cuda:0')
(Pdb) p prev_actions[201,0]
tensor([0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:0')
(Pdb) p prev_actions[202,0]
tensor([-0.000,  0.002,  0.001,  0.004, -0.001,  0.001, -0.004,  0.005],
       device='cuda:0')
(Pdb) p prev_actions[165,0]
tensor([ 0.025, -0.251, -0.005,  0.035, -0.196, -0.194, -0.142,  0.071],
       device='cuda:0')
(Pdb) p prev_actions[203,0]
tensor([-0.002,  0.002,  0.000,  0.004, -0.001,  0.002, -0.007,  0.006],
       device='cuda:0')
(Pdb) p prev_actions[204,0]
tensor([-0.002,  0.003, -0.000,  0.004, -0.001,  0.002, -0.006,  0.006],
       device='cuda:0')
(Pdb) p obs[201,0]
tensor([ 0.704,  0.994, -0.068, -0.075,  0.044,  0.025,  0.024,  0.079, -0.012,
        -0.002, -0.021, -0.089,  0.027,  0.014,  0.042, -0.095,  0.039, -0.025,
        -0.061,  0.106,  0.274, -0.117,  0.080, -0.088, -0.054,  0.045, -0.027,
         0.000], device='cuda:0')
(Pdb) p obs[202,0]
tensor([     0.719,      0.994,     -0.068,     -0.072,      0.042,      0.031,
             0.431,      0.073,     -0.426,     -0.007,     -0.428,     -0.086,
             0.429,     -0.093,      0.125,      0.078,     -0.019,      0.122,
            -0.060,      0.110,      8.957,     -0.118,     -9.142,     -0.111,
            -8.964,      0.041,      8.896,      0.000], device='cuda:0')
(Pdb) p obs[203,0]
tensor([ 0.706,  0.994, -0.069, -0.069,  0.040,  0.036,  0.868,  0.067, -0.871,
        -0.013, -0.864, -0.085,  0.863, -0.065,  0.097, -0.629, -0.016,  0.098,
        -0.065,  0.107,  8.550, -0.118, -8.681, -0.154, -8.483,  0.025,  8.477,
         0.000], device='cuda:0')
(Pdb) 








Confirmed healthy 2 :D

Updated 0/81 times.
> /mmfs1/gscratch/weirdlab/qirico/Meta-Learning-25-10-1/weirdl-pomdp-baselines-cp38/policies/models/policy_transformer.py(211)forward()
-> policy_loss = (policy_loss * masks).sum() / num_valid
(Pdb) p actions[200:202,0]
tensor([[-0.031,  0.541,  0.248, -0.339, -0.524,  0.568, -0.236, -0.390],
        [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000]],
       device='cuda:0')
(Pdb) p actions[200:203,0]
tensor([[-0.031,  0.541,  0.248, -0.339, -0.524,  0.568, -0.236, -0.390],
        [ 0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000,  0.000],
        [-0.356, -0.682, -0.651,  0.420, -0.884, -0.167,  0.665,  0.806]],
       device='cuda:0')
(Pdb) p observs[200:203,0]
tensor([[     0.612,      0.962,     -0.064,      0.056,     -0.258,      0.356,
              0.516,      0.423,     -1.144,     -0.360,     -1.229,      0.353,
              1.227,      0.078,     -0.182,     -0.236,     -0.723,      0.034,
              0.708,      1.953,     -0.390,     -3.871,      2.987,     -2.183,
             -1.522,     -1.694,     -0.112,      1.000],
        [     0.787,      0.994,     -0.078,      0.065,     -0.028,      0.042,
             -0.066,      0.071,      0.058,     -0.004,      0.076,      0.000,
              0.034,      0.093,     -0.114,      0.203,     -0.018,      0.036,
              0.042,      0.113,      0.144,      0.248,      0.119,      0.198,
              0.250,      0.045,      0.001,      0.000],
        [     0.822,      0.995,     -0.069,      0.073,     -0.011,     -0.072,
              0.444,     -0.077,     -0.424,      0.125,     -0.363,     -0.062,
              0.392,     -0.221,      0.207,      0.455,      0.911,      0.269,
              1.354,     -4.623,     12.517,     -6.138,    -11.184,      4.931,
             -7.491,     -2.542,      6.027,      0.000]], device='cuda:0')
(Pdb) p rewards[200:203,0]
tensor([[ 0.096],
        [ 0.000],
        [-2.652]], device='cuda:0')
(Pdb) p nominals[200:203,0]
tensor([[0],
        [1],
        [1]], device='cuda:0')
(Pdb) p qf1_loss[200:203,0]
*** IndexError: too many indices for tensor of dimension 0
(Pdb) p masks[200:203,0]
tensor([[0.],
        [1.],
        [1.]], device='cuda:0')
(Pdb) p qf1_loss.shape
torch.Size([])
(Pdb) p qf1_loss
tensor(6.691, device='cuda:0', grad_fn=<DivBackward0>)
(Pdb) p q1_pred.shape
torch.Size([601, 18, 1])
(Pdb) p q1_pred[200:203,0]
tensor([[0.000],
        [0.015],
        [0.021]], device='cuda:0', grad_fn=<SelectBackward>)
(Pdb) p q1_pred[198:203,0]
tensor([[0.000],
        [0.000],
        [0.000],
        [0.015],
        [0.021]], device='cuda:0', grad_fn=<SelectBackward>)