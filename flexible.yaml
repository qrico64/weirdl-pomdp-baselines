seed: 40
cuda: 0 # use_gpu
env:
  env_type: meta
  env_name: PegInsertion-v0
  max_rollouts_per_task: 2 # k=2, H=200, H^+ =400

  num_tasks: 36
  num_train_tasks: 16
  num_eval_tasks: 20
  task_mode: fixed
  reward_conditioning: no
  goal_conditioning: yes_both
  normalize_kwarg: true
  num_parallel_workers: 20
  reward_scale: 2.0
  goal_radius: 0.4
  infinite_tasks: yes

  eval_env:
    goal_noise_magnitude: 0

train:
  # sample complexity: BAMDP horizon * (num_init_rollouts_pool * num_train_tasks
    #  + num_iters * num_tasks_sample * num_rollouts_per_iter)
    # 2k iters -> 20M steps
  # original rl training steps: num_iters * updates_per_iter = 1M
    # now makes it same as env steps
  num_iters: 3000
  num_init_rollouts_pool: 50 # before training
  num_rollouts_per_iter: 20  #
  buffer_size: 9e6

  num_updates_per_iter: 0.01 # equal, or positive integer
  batch_size: 18 # to tune based on sampled_seq_len
  sampled_seq_len: -1 # -1 is all, or positive integer
  sample_weight_baseline: 0.0

  use_residuals: true
  base_model_config_file: metaworld_expert_peg_insertion

  # Data collection mode - if true, just collect data using base_model, no training
  collect_data_only: true
  num_data_collection_rollouts: 1000  # total rollouts to collect in data-only mode
  data_collection_action_noise: 0.0  # std of Gaussian noise added to base policy actions
  data_collection_save_interval: 20  # save buffer every k rollouts

eval:
  eval_stochastic: false # also eval stochastic policy
  log_interval: 1 # num of iters
  save_interval: 512
  log_tensorboard: true

policy:
  seq_model: transformer # [lstm, gru]
  feature_extractor_type: combined2
  algo_name: sac # [td3, sac]

  action_embedding_size: 128
  observ_embedding_size: 128
  reward_embedding_size: 128
  nominal_embedding_size: 16
  rnn_num_layers: 3

  dqn_layers: [256, 256]
  policy_layers: [256, 256]
  lr: 0.0003
  gamma: 0.99
  tau: 0.005

  sac:
    entropy_alpha: 0.2
    automatic_entropy_tuning: true
    alpha_lr: 0.0003

  td3:
    ## since we normalize action space to [-1, 1]
    ## the noise std is absolute value
    exploration_noise: 0.1
    target_noise: 0.2
    target_noise_clip: 0.5
